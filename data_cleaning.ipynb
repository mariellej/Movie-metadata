{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use an n-gram vectorizer to collect significant words and phrases\n",
    "from the keywords column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv('movie_metadata.csv', dtype = str)\n",
    "\n",
    "#blank entries replaced with space to avoid errors\n",
    "corpus = data['plot_keywords'].fillna(\" \")\n",
    "\n",
    "\n",
    "st = PorterStemmer()\n",
    "corpus = corpus.apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))#stem words\n",
    "\n",
    "\"\"\" tokenize all words 3 letters or longer.  \n",
    "    Ignore 1-grams that occur in more than 60% \n",
    "    or in less than 2% of descriptions\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer(stop_words = 'english', strip_accents=ascii, analyzer = 'word',max_df=0.75, min_df=.01)\n",
    "bow = np.array(vectorizer.fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    actor  agent  alien  american  base  battle  blood  book  box  boy  ...   \\\n",
      "0       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "1       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "2       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "3       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "4       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "5       0      0      1         1     0       0      0     0    0    0  ...    \n",
      "6       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "7       0      0      0         0     0       0      0     0    0    0  ...    \n",
      "8       0      0      0         0     0       0      0     1    0    0  ...    \n",
      "9       0      0      0         0     0       0      1     1    0    0  ...    \n",
      "10      0      0      0         0     1       0      0     1    0    0  ...    \n",
      "11      0      0      0         0     0       0      0     0    0    0  ...    \n",
      "12      0      0      0         0     0       0      0     0    0    0  ...    \n",
      "13      0      0      0         0     0       0      0     0    1    0  ...    \n",
      "14      0      0      0         0     0       0      0     0    0    0  ...    \n",
      "15      1      0      0         1     1       1      0     1    0    0  ...    \n",
      "16      0      0      0         0     0       0      0     0    0    0  ...    \n",
      "17      0      0      1         0     0       1      0     0    0    0  ...    \n",
      "18      0      0      0         0     0       0      0     0    0    0  ...    \n",
      "19      0      0      1         0     0       0      0     0    0    0  ...    \n",
      "\n",
      "    title  town  train  travel  war  wed  woman  word  year  york  \n",
      "0       0     0      0       0    0    0      0     0     0     0  \n",
      "1       0     0      0       0    0    0      0     0     0     0  \n",
      "2       0     0      0       0    0    0      0     0     0     0  \n",
      "3       0     0      0       0    0    0      0     0     0     0  \n",
      "4       0     0      0       0    0    0      0     0     0     0  \n",
      "5       0     0      0       0    1    0      0     0     0     0  \n",
      "6       0     0      0       0    0    0      0     0     0     0  \n",
      "7       0     0      0       0    0    0      0     0     0     0  \n",
      "8       0     0      0       0    0    0      0     0     0     0  \n",
      "9       0     0      0       0    0    0      0     0     0     0  \n",
      "10      0     0      0       0    0    0      0     0     0     0  \n",
      "11      0     0      0       0    0    0      0     0     0     0  \n",
      "12      0     0      0       0    0    0      0     0     0     0  \n",
      "13      0     0      0       0    0    0      0     0     0     0  \n",
      "14      0     0      1       0    0    0      0     0     0     0  \n",
      "15      0     0      0       0    0    0      0     0     0     0  \n",
      "16      0     0      0       0    0    0      0     0     0     0  \n",
      "17      0     0      0       0    0    0      0     0     0     0  \n",
      "18      0     0      0       0    0    0      0     0     0     0  \n",
      "19      0     0      0       0    0    0      0     0     0     0  \n",
      "\n",
      "[20 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "append plot_keywords document term matrix to the original data set\n",
    "\"\"\"\n",
    "keywords_dtm = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "print(keywords_dtm.head(20))\n",
    "data_with_keywords = pd.concat([data, keywords_dtm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   color      director_name num_critic_for_reviews duration  \\\n",
      "0  Color      James Cameron                    723      178   \n",
      "1  Color     Gore Verbinski                    302      169   \n",
      "2  Color         Sam Mendes                    602      148   \n",
      "3  Color  Christopher Nolan                    813      164   \n",
      "4    NaN        Doug Walker                    NaN      NaN   \n",
      "\n",
      "  director_facebook_likes actor_3_facebook_likes      actor_2_name  \\\n",
      "0                       0                    855  Joel David Moore   \n",
      "1                     563                   1000     Orlando Bloom   \n",
      "2                       0                    161      Rory Kinnear   \n",
      "3                   22000                  23000    Christian Bale   \n",
      "4                     131                    NaN        Rob Walker   \n",
      "\n",
      "  actor_1_facebook_likes      gross                           genres   ...    \\\n",
      "0                   1000  760505847  Action|Adventure|Fantasy|Sci-Fi   ...     \n",
      "1                  40000  309404152         Action|Adventure|Fantasy   ...     \n",
      "2                  11000  200074175        Action|Adventure|Thriller   ...     \n",
      "3                  27000  448130642                  Action|Thriller   ...     \n",
      "4                    131        NaN                      Documentary   ...     \n",
      "\n",
      "  Sci-Fi Reality-TV History Documentary Romance Sport War Drama Short Western  \n",
      "0      1          0       0           0       0     0   0     0     0       0  \n",
      "1      0          0       0           0       0     0   0     0     0       0  \n",
      "2      0          0       0           0       0     0   0     0     0       0  \n",
      "3      0          0       0           0       0     0   0     0     0       0  \n",
      "4      0          0       0           1       0     0   0     0     0       0  \n",
      "\n",
      "[5 rows x 125 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"extracting individual genre tags from genre column\"\"\"\n",
    "\n",
    "#no blanks in genres column, no need to fill.na\n",
    "\n",
    "genres_list= data[\"genres\"].str.split(\"|\", expand = False)\n",
    "genre_set = set(x for l in genres_list for x in l)\n",
    "index = range(len(genres_list))\n",
    "genre_df = pd.DataFrame(index = index,columns = genre_set)\n",
    "genre_df = genre_df.fillna(0)\n",
    "\n",
    "#creating term matrix for genres\n",
    "index = 0\n",
    "for l in genres_list:\n",
    "    for g in l:\n",
    "        genre_df.at[index,g] = 1\n",
    "    index= index +1\n",
    "  \n",
    "#append\n",
    "final_data = pd.concat([data_with_keywords,genre_df],axis=1)\n",
    "\n",
    "print(final_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export new data to csv\n",
    "final_data.to_csv(\"cleaned_movie_data.csv\", header= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
